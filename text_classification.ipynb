{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Classification with Keras Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading data from files to Python variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = load_files('/home/hadoop/scikit_learn_data/20news_home/20news-bydate-train',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: gnelson@pion.rutgers.edu (Gregory Nelson)\\nSubject: Thanks Apple: Free Ethernet on my C610!\\nArticle-I.D.: pion.Apr.6.12.05.34.1993.11732\\nOrganization: Rutgers Univ., New Brunswick, N.J.\\nLines: 26\\n\\n\\n\\tWell, I just got my Centris 610 yesterday.  It took just over two \\nweeks from placing the order.  The dealer (Rutgers computer store) \\nappologized because Apple made a substitution on my order.  I ordered\\nthe one without ethernet, but they substituted one _with_ ethernet.\\nHe wanted to know if that would be \"alright with me\"!!!  They must\\nbe backlogged on Centri w/out ethernet so they\\'re just shipping them\\nwith!  \\n\\n\\tAnyway, I\\'m very happy with the 610 with a few exceptions.  \\nBeing nosy, I decided to open it up _before_ powering it on for the first\\ntime.  The SCSI cable to the hard drive was only partially connected\\n(must have come loose in shipping).  No big deal, but I would have been\\npissed if I tried to boot it and it wouldn\\'t come up!\\n\\tThe hard drive also has an annoying high pitched whine.  I\\'ve\\nheard apple will exchange it if you complain, so I might try to get\\nit swapped.\\n\\tI am also dissappionted by the lack of soft power-on/off.  This\\nwasn\\'t mentioned in any of the literature I saw.  Also, the location\\nof the reset/interupt buttons is awful.  Having keyboard control for\\nthese functions was much more convenient.\\n\\tOh, and the screen seems tojump in a wierd way on power-up.\\nI\\'ve seen this mentioned by others, so it must be a...feature...\\n\\tAnyway, above all, it\\'s fast.  A great machine at a great price!\\n\\ngnelson@physics.rutgers.edu\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.sys.mac.hardware'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[twenty_train.target[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = twenty_train.data # Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = twenty_train.target # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load tools we need for preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 #define vocabulary size is 20.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trains = tokenizer.texts_to_matrix(texts, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.90025907, 1.26800369, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'to',\n",
       " 3: 'of',\n",
       " 4: 'a',\n",
       " 5: \"'ax\",\n",
       " 6: 'and',\n",
       " 7: 'in',\n",
       " 8: 'i',\n",
       " 9: 'is',\n",
       " 10: 'that',\n",
       " 11: 'it',\n",
       " 12: 'for',\n",
       " 13: 'you',\n",
       " 14: 'from',\n",
       " 15: 'edu',\n",
       " 16: 'on',\n",
       " 17: 'this',\n",
       " 18: 'be',\n",
       " 19: 'are',\n",
       " 20: 'not',\n",
       " 21: 'have',\n",
       " 22: 'with',\n",
       " 23: 'as',\n",
       " 24: '1',\n",
       " 25: 'or',\n",
       " 26: 'was',\n",
       " 27: 'if',\n",
       " 28: 'but',\n",
       " 29: 'subject',\n",
       " 30: 'they',\n",
       " 31: 'com',\n",
       " 32: 'lines',\n",
       " 33: 'at',\n",
       " 34: 'organization',\n",
       " 35: 'by',\n",
       " 36: '2',\n",
       " 37: 'an',\n",
       " 38: 'my',\n",
       " 39: 'can',\n",
       " 40: 'x',\n",
       " 41: '3',\n",
       " 42: 'what',\n",
       " 43: '0',\n",
       " 44: 'all',\n",
       " 45: 'will',\n",
       " 46: 'm',\n",
       " 47: 'there',\n",
       " 48: 'would',\n",
       " 49: 'one',\n",
       " 50: 'do',\n",
       " 51: \"'\",\n",
       " 52: 'about',\n",
       " 53: 're',\n",
       " 54: 'we',\n",
       " 55: 'writes',\n",
       " 56: 'so',\n",
       " 57: 'he',\n",
       " 58: 'your',\n",
       " 59: 'no',\n",
       " 60: 'has',\n",
       " 61: 'article',\n",
       " 62: 'any',\n",
       " 63: 'me',\n",
       " 64: 'some',\n",
       " 65: 'who',\n",
       " 66: 'out',\n",
       " 67: 'which',\n",
       " 68: '4',\n",
       " 69: 'q',\n",
       " 70: 'more',\n",
       " 71: 'like',\n",
       " 72: 'people',\n",
       " 73: \"don't\",\n",
       " 74: 'when',\n",
       " 75: '5',\n",
       " 76: 'just',\n",
       " 77: 'university',\n",
       " 78: 'posting',\n",
       " 79: 'their',\n",
       " 80: 'were',\n",
       " 81: 'up',\n",
       " 82: 'r',\n",
       " 83: 'p',\n",
       " 84: 'w',\n",
       " 85: 'how',\n",
       " 86: 'other',\n",
       " 87: '7',\n",
       " 88: 'know',\n",
       " 89: 's',\n",
       " 90: 'only',\n",
       " 91: 'host',\n",
       " 92: 'get',\n",
       " 93: 'c',\n",
       " 94: 'them',\n",
       " 95: 'nntp',\n",
       " 96: 'max',\n",
       " 97: 'than',\n",
       " 98: 'had',\n",
       " 99: 'think',\n",
       " 100: 'g',\n",
       " 101: 'been',\n",
       " 102: 'his',\n",
       " 103: '8',\n",
       " 104: 'o',\n",
       " 105: 'also',\n",
       " 106: '6',\n",
       " 107: 'use',\n",
       " 108: 'does',\n",
       " 109: 'time',\n",
       " 110: 'new',\n",
       " 111: 'then',\n",
       " 112: 'e',\n",
       " 113: \"it's\",\n",
       " 114: 'good',\n",
       " 115: \"i'm\",\n",
       " 116: 'these',\n",
       " 117: 'd',\n",
       " 118: 'u',\n",
       " 119: 'should',\n",
       " 120: '9',\n",
       " 121: 'ca',\n",
       " 122: 'n',\n",
       " 123: 'could',\n",
       " 124: 'well',\n",
       " 125: 'us',\n",
       " 126: 'because',\n",
       " 127: 'am',\n",
       " 128: 'b',\n",
       " 129: 'may',\n",
       " 130: 't',\n",
       " 131: 'even',\n",
       " 132: 'why',\n",
       " 133: 'very',\n",
       " 134: 'now',\n",
       " 135: 'into',\n",
       " 136: 'see',\n",
       " 137: 'cs',\n",
       " 138: 'two',\n",
       " 139: 'way',\n",
       " 140: 'v',\n",
       " 141: 'first',\n",
       " 142: 'many',\n",
       " 143: 'those',\n",
       " 144: 'make',\n",
       " 145: 'much',\n",
       " 146: 'most',\n",
       " 147: 'system',\n",
       " 148: 'such',\n",
       " 149: 'distribution',\n",
       " 150: 'right',\n",
       " 151: 'say',\n",
       " 152: 'l',\n",
       " 153: 'where',\n",
       " 154: 'world',\n",
       " 155: 'god',\n",
       " 156: 'k',\n",
       " 157: 'z',\n",
       " 158: 'h',\n",
       " 159: 'want',\n",
       " 160: 'our',\n",
       " 161: 'here',\n",
       " 162: 'its',\n",
       " 163: 'go',\n",
       " 164: '10',\n",
       " 165: 'reply',\n",
       " 166: 'used',\n",
       " 167: 'said',\n",
       " 168: 'being',\n",
       " 169: 'did',\n",
       " 170: 'over',\n",
       " 171: 'anyone',\n",
       " 172: 'same',\n",
       " 173: 'after',\n",
       " 174: 'need',\n",
       " 175: 'work',\n",
       " 176: 'too',\n",
       " 177: 'state',\n",
       " 178: 'something',\n",
       " 179: 'problem',\n",
       " 180: 'j',\n",
       " 181: 'please',\n",
       " 182: 'really',\n",
       " 183: 'computer',\n",
       " 184: 'him',\n",
       " 185: 'off',\n",
       " 186: 'since',\n",
       " 187: 'f',\n",
       " 188: 'mail',\n",
       " 189: 'believe',\n",
       " 190: 'still',\n",
       " 191: 'back',\n",
       " 192: 'going',\n",
       " 193: 'file',\n",
       " 194: 'year',\n",
       " 195: 'information',\n",
       " 196: 'windows',\n",
       " 197: 'help',\n",
       " 198: 'years',\n",
       " 199: 'using',\n",
       " 200: 'find',\n",
       " 201: 'take',\n",
       " 202: 'question',\n",
       " 203: 'point',\n",
       " 204: 'last',\n",
       " 205: 'space',\n",
       " 206: 'thanks',\n",
       " 207: 'before',\n",
       " 208: 'must',\n",
       " 209: \"i've\",\n",
       " 210: 'never',\n",
       " 211: '15',\n",
       " 212: 'things',\n",
       " 213: 'better',\n",
       " 214: 'while',\n",
       " 215: '16',\n",
       " 216: 'news',\n",
       " 217: '20',\n",
       " 218: 'government',\n",
       " 219: 'might',\n",
       " 220: 'usa',\n",
       " 221: 'own',\n",
       " 222: \"can't\",\n",
       " 223: 'both',\n",
       " 224: 'number',\n",
       " 225: 'read',\n",
       " 226: 'sure',\n",
       " 227: 'program',\n",
       " 228: 'another',\n",
       " 229: 'case',\n",
       " 230: 'without',\n",
       " 231: 'etc',\n",
       " 232: 'key',\n",
       " 233: 'david',\n",
       " 234: 'data',\n",
       " 235: 'down',\n",
       " 236: 'through',\n",
       " 237: 'got',\n",
       " 238: 'y',\n",
       " 239: 'made',\n",
       " 240: 'drive',\n",
       " 241: '14',\n",
       " 242: 'software',\n",
       " 243: 'bit',\n",
       " 244: '1993',\n",
       " 245: 'long',\n",
       " 246: 'available',\n",
       " 247: 'law',\n",
       " 248: 'under',\n",
       " 249: '00',\n",
       " 250: 'thing',\n",
       " 251: '12',\n",
       " 252: 'someone',\n",
       " 253: \"doesn't\",\n",
       " 254: 'power',\n",
       " 255: 'look',\n",
       " 256: 'part',\n",
       " 257: 'uk',\n",
       " 258: 'between',\n",
       " 259: 'few',\n",
       " 260: 'little',\n",
       " 261: 'version',\n",
       " 262: 'come',\n",
       " 263: 'day',\n",
       " 264: \"that's\",\n",
       " 265: \"didn't\",\n",
       " 266: 'however',\n",
       " 267: '25',\n",
       " 268: 'each',\n",
       " 269: 'public',\n",
       " 270: 'org',\n",
       " 271: 'anything',\n",
       " 272: 'around',\n",
       " 273: 'name',\n",
       " 274: 'fact',\n",
       " 275: 'science',\n",
       " 276: 'give',\n",
       " 277: 'john',\n",
       " 278: 'cc',\n",
       " 279: 'access',\n",
       " 280: 'every',\n",
       " 281: 'best',\n",
       " 282: 'true',\n",
       " 283: 'probably',\n",
       " 284: 'again',\n",
       " 285: '17',\n",
       " 286: '11',\n",
       " 287: 'line',\n",
       " 288: '30',\n",
       " 289: 'research',\n",
       " 290: 'gov',\n",
       " 291: 'against',\n",
       " 292: 'nasa',\n",
       " 293: 'course',\n",
       " 294: 'least',\n",
       " 295: 'tell',\n",
       " 296: 'set',\n",
       " 297: 'seems',\n",
       " 298: 'car',\n",
       " 299: 'different',\n",
       " 300: 'group',\n",
       " 301: 'list',\n",
       " 302: 'great',\n",
       " 303: 'systems',\n",
       " 304: 'put',\n",
       " 305: 'enough',\n",
       " 306: 'run',\n",
       " 307: 'high',\n",
       " 308: 'try',\n",
       " 309: '24',\n",
       " 310: 'hard',\n",
       " 311: 'lot',\n",
       " 312: 'ac',\n",
       " 313: 'real',\n",
       " 314: 'says',\n",
       " 315: 'second',\n",
       " 316: '18',\n",
       " 317: 'life',\n",
       " 318: 'mr',\n",
       " 319: 'far',\n",
       " 320: '13',\n",
       " 321: 'old',\n",
       " 322: 'net',\n",
       " 323: 'possible',\n",
       " 324: 'g9v',\n",
       " 325: 'actually',\n",
       " 326: 'either',\n",
       " 327: 'end',\n",
       " 328: 'post',\n",
       " 329: 'game',\n",
       " 330: 'though',\n",
       " 331: 'support',\n",
       " 332: 'gun',\n",
       " 333: 'inc',\n",
       " 334: 'technology',\n",
       " 335: 'card',\n",
       " 336: 'b8f',\n",
       " 337: 'called',\n",
       " 338: 'her',\n",
       " 339: 'she',\n",
       " 340: 'de',\n",
       " 341: \"i'd\",\n",
       " 342: 'sun',\n",
       " 343: 'free',\n",
       " 344: 'rather',\n",
       " 345: 'window',\n",
       " 346: 'center',\n",
       " 347: 'email',\n",
       " 348: 'nothing',\n",
       " 349: 'internet',\n",
       " 350: '19',\n",
       " 351: 'next',\n",
       " 352: 'non',\n",
       " 353: 'team',\n",
       " 354: 'chip',\n",
       " 355: 'mean',\n",
       " 356: 'jesus',\n",
       " 357: 'info',\n",
       " 358: 'let',\n",
       " 359: 'problems',\n",
       " 360: \"you're\",\n",
       " 361: 'call',\n",
       " 362: 'wrong',\n",
       " 363: 'keep',\n",
       " 364: 'files',\n",
       " 365: 'send',\n",
       " 366: 'based',\n",
       " 367: 'apr',\n",
       " 368: '50',\n",
       " 369: 'message',\n",
       " 370: 'bad',\n",
       " 371: 'mark',\n",
       " 372: 'order',\n",
       " 373: 'example',\n",
       " 374: 'reason',\n",
       " 375: 'a86',\n",
       " 376: 'having',\n",
       " 377: 'yes',\n",
       " 378: 'done',\n",
       " 379: 'found',\n",
       " 380: 'maybe',\n",
       " 381: 'able',\n",
       " 382: 'else',\n",
       " 383: 'netcom',\n",
       " 384: 'above',\n",
       " 385: '21',\n",
       " 386: 'looking',\n",
       " 387: 'man',\n",
       " 388: 'national',\n",
       " 389: 'person',\n",
       " 390: 'control',\n",
       " 391: 'thought',\n",
       " 392: \"isn't\",\n",
       " 393: 'three',\n",
       " 394: 'hp',\n",
       " 395: 'ibm',\n",
       " 396: 'always',\n",
       " 397: 'less',\n",
       " 398: 'mit',\n",
       " 399: 'place',\n",
       " 400: 'others',\n",
       " 401: '145',\n",
       " 402: 'following',\n",
       " 403: 'left',\n",
       " 404: 'times',\n",
       " 405: '22',\n",
       " 406: 'doing',\n",
       " 407: 'general',\n",
       " 408: 'scsi',\n",
       " 409: 'bill',\n",
       " 410: 'several',\n",
       " 411: 'trying',\n",
       " 412: 'keywords',\n",
       " 413: 'code',\n",
       " 414: 'seen',\n",
       " 415: 'dos',\n",
       " 416: '000',\n",
       " 417: 'uucp',\n",
       " 418: 'heard',\n",
       " 419: 'graphics',\n",
       " 420: 'yet',\n",
       " 421: 'american',\n",
       " 422: 'ever',\n",
       " 423: 'means',\n",
       " 424: 'home',\n",
       " 425: '93',\n",
       " 426: 'quite',\n",
       " 427: 'christian',\n",
       " 428: 'phone',\n",
       " 429: 'away',\n",
       " 430: 'once',\n",
       " 431: 'big',\n",
       " 432: 'questions',\n",
       " 433: 'israel',\n",
       " 434: 'note',\n",
       " 435: 'wrote',\n",
       " 436: 'win',\n",
       " 437: 'opinions',\n",
       " 438: 'given',\n",
       " 439: 'idea',\n",
       " 440: 'steve',\n",
       " 441: 'washington',\n",
       " 442: 'mac',\n",
       " 443: 'getting',\n",
       " 444: 'start',\n",
       " 445: 'play',\n",
       " 446: 'source',\n",
       " 447: '23',\n",
       " 448: 'whether',\n",
       " 449: 'human',\n",
       " 450: 'jews',\n",
       " 451: 'during',\n",
       " 452: 'pc',\n",
       " 453: '40',\n",
       " 454: 'book',\n",
       " 455: 'today',\n",
       " 456: 'standard',\n",
       " 457: 'michael',\n",
       " 458: 'kind',\n",
       " 459: 'current',\n",
       " 460: 'remember',\n",
       " 461: 'encryption',\n",
       " 462: 'already',\n",
       " 463: 'money',\n",
       " 464: 'ftp',\n",
       " 465: 'pl',\n",
       " 466: 'seem',\n",
       " 467: \"i'll\",\n",
       " 468: 'au',\n",
       " 469: 'cannot',\n",
       " 470: 'local',\n",
       " 471: 'until',\n",
       " 472: 'disk',\n",
       " 473: 'image',\n",
       " 474: 'ask',\n",
       " 475: 'makes',\n",
       " 476: 'andrew',\n",
       " 477: 'games',\n",
       " 478: 'large',\n",
       " 479: 'co',\n",
       " 480: 'evidence',\n",
       " 481: '32',\n",
       " 482: 'department',\n",
       " 483: 'address',\n",
       " 484: 'small',\n",
       " 485: 'word',\n",
       " 486: 'fax',\n",
       " 487: 'speed',\n",
       " 488: 'president',\n",
       " 489: 'apple',\n",
       " 490: 'change',\n",
       " 491: 'ago',\n",
       " 492: 'institute',\n",
       " 493: '27',\n",
       " 494: 'clipper',\n",
       " 495: 'perhaps',\n",
       " 496: '34',\n",
       " 497: 'answer',\n",
       " 498: 'mike',\n",
       " 499: 'jim',\n",
       " 500: 'issue',\n",
       " 501: 'bible',\n",
       " 502: 'told',\n",
       " 503: 'open',\n",
       " 504: 'running',\n",
       " 505: 'price',\n",
       " 506: 'full',\n",
       " 507: 'memory',\n",
       " 508: 'works',\n",
       " 509: 'came',\n",
       " 510: 'robert',\n",
       " 511: 'hand',\n",
       " 512: 'unix',\n",
       " 513: 'rights',\n",
       " 514: 'whole',\n",
       " 515: 'children',\n",
       " 516: 'sale',\n",
       " 517: '26',\n",
       " 518: 'buy',\n",
       " 519: 'interested',\n",
       " 520: 'april',\n",
       " 521: '100',\n",
       " 522: 'pretty',\n",
       " 523: 'type',\n",
       " 524: 'uiuc',\n",
       " 525: 'server',\n",
       " 526: 'stuff',\n",
       " 527: 'live',\n",
       " 528: 'turkish',\n",
       " 529: 'show',\n",
       " 530: 'hope',\n",
       " 531: 'states',\n",
       " 532: \"there's\",\n",
       " 533: 'days',\n",
       " 534: 'color',\n",
       " 535: 'paul',\n",
       " 536: 'side',\n",
       " 537: 'saying',\n",
       " 538: 'vs',\n",
       " 539: 'everything',\n",
       " 540: 'war',\n",
       " 541: 'canada',\n",
       " 542: 'important',\n",
       " 543: 'armenian',\n",
       " 544: 'original',\n",
       " 545: '28',\n",
       " 546: 'oh',\n",
       " 547: 'check',\n",
       " 548: 'went',\n",
       " 549: 'toronto',\n",
       " 550: '1d9',\n",
       " 551: 'box',\n",
       " 552: 'matter',\n",
       " 553: 'machine',\n",
       " 554: 'agree',\n",
       " 555: 'video',\n",
       " 556: '0d',\n",
       " 557: 'cost',\n",
       " 558: 'white',\n",
       " 559: 'including',\n",
       " 560: 'often',\n",
       " 561: 'israeli',\n",
       " 562: 'comes',\n",
       " 563: 'christians',\n",
       " 564: 'area',\n",
       " 565: 'feel',\n",
       " 566: 'faq',\n",
       " 567: 'house',\n",
       " 568: 'everyone',\n",
       " 569: 'almost',\n",
       " 570: 'colorado',\n",
       " 571: 'simply',\n",
       " 572: 'wanted',\n",
       " 573: 'understand',\n",
       " 574: 'ms',\n",
       " 575: \"won't\",\n",
       " 576: 'later',\n",
       " 577: 'output',\n",
       " 578: 'security',\n",
       " 579: 'programs',\n",
       " 580: \"wouldn't\",\n",
       " 581: 'hockey',\n",
       " 582: 'care',\n",
       " 583: 'display',\n",
       " 584: 'engineering',\n",
       " 585: 'love',\n",
       " 586: 'single',\n",
       " 587: 'working',\n",
       " 588: 'include',\n",
       " 589: 'dept',\n",
       " 590: 'low',\n",
       " 591: 'mind',\n",
       " 592: 'cmu',\n",
       " 593: 'service',\n",
       " 594: 'st',\n",
       " 595: 'claim',\n",
       " 596: 'db',\n",
       " 597: 'although',\n",
       " 598: 'instead',\n",
       " 599: 'top',\n",
       " 600: 'hi',\n",
       " 601: 'certainly',\n",
       " 602: 'truth',\n",
       " 603: 'write',\n",
       " 604: 'ohio',\n",
       " 605: 'ok',\n",
       " 606: 'church',\n",
       " 607: 'anyway',\n",
       " 608: 'california',\n",
       " 609: 'school',\n",
       " 610: 'college',\n",
       " 611: '45',\n",
       " 612: 'consider',\n",
       " 613: '31',\n",
       " 614: '34u',\n",
       " 615: 'season',\n",
       " 616: 'guess',\n",
       " 617: 'started',\n",
       " 618: 'size',\n",
       " 619: 'anybody',\n",
       " 620: 'known',\n",
       " 621: 'provide',\n",
       " 622: 'hell',\n",
       " 623: 'armenians',\n",
       " 624: 'religion',\n",
       " 625: 'contact',\n",
       " 626: 'package',\n",
       " 627: 'men',\n",
       " 628: 'history',\n",
       " 629: 'unless',\n",
       " 630: 'division',\n",
       " 631: 'making',\n",
       " 632: 'tried',\n",
       " 633: 'city',\n",
       " 634: 'keys',\n",
       " 635: 'earth',\n",
       " 636: 'san',\n",
       " 637: '33',\n",
       " 638: 'cause',\n",
       " 639: 'country',\n",
       " 640: 'network',\n",
       " 641: 'death',\n",
       " 642: 'james',\n",
       " 643: 'similar',\n",
       " 644: 'summary',\n",
       " 645: 'newsreader',\n",
       " 646: 'talk',\n",
       " 647: 'jewish',\n",
       " 648: 'fast',\n",
       " 649: 'hardware',\n",
       " 650: 'certain',\n",
       " 651: 'services',\n",
       " 652: 'user',\n",
       " 653: 'difference',\n",
       " 654: 'private',\n",
       " 655: '80',\n",
       " 656: 'players',\n",
       " 657: 'couple',\n",
       " 658: 'level',\n",
       " 659: 'took',\n",
       " 660: 'company',\n",
       " 661: 'na',\n",
       " 662: 'numbers',\n",
       " 663: 'black',\n",
       " 664: '29',\n",
       " 665: 'sort',\n",
       " 666: 'screen',\n",
       " 667: 'pub',\n",
       " 668: 'within',\n",
       " 669: 'major',\n",
       " 670: 'faith',\n",
       " 671: 'view',\n",
       " 672: 'police',\n",
       " 673: 'clinton',\n",
       " 674: \"he's\",\n",
       " 675: '0t',\n",
       " 676: 'likely',\n",
       " 677: 'correct',\n",
       " 678: 'per',\n",
       " 679: 'argument',\n",
       " 680: '35',\n",
       " 681: 'nice',\n",
       " 682: 'talking',\n",
       " 683: 'columbia',\n",
       " 684: 'cx',\n",
       " 685: 'berkeley',\n",
       " 686: 'via',\n",
       " 687: 'pitt',\n",
       " 688: \"they're\",\n",
       " 689: 'driver',\n",
       " 690: 'bike',\n",
       " 691: 'pay',\n",
       " 692: 'asked',\n",
       " 693: 'dave',\n",
       " 694: 'health',\n",
       " 695: 'ma',\n",
       " 696: 'york',\n",
       " 697: 'text',\n",
       " 698: 'guns',\n",
       " 699: 'application',\n",
       " 700: 'usually',\n",
       " 701: 'board',\n",
       " 702: 'experience',\n",
       " 703: 'press',\n",
       " 704: 'exactly',\n",
       " 705: '55',\n",
       " 706: 'virginia',\n",
       " 707: 'period',\n",
       " 708: 'saw',\n",
       " 709: 'cwru',\n",
       " 710: 'words',\n",
       " 711: 'keith',\n",
       " 712: 'especially',\n",
       " 713: 'sound',\n",
       " 714: 'sense',\n",
       " 715: 'groups',\n",
       " 716: 'al',\n",
       " 717: 'simple',\n",
       " 718: 'themselves',\n",
       " 719: 'points',\n",
       " 720: 'att',\n",
       " 721: 'sorry',\n",
       " 722: 'reading',\n",
       " 723: 'office',\n",
       " 724: \"we're\",\n",
       " 725: 'copy',\n",
       " 726: 'gmt',\n",
       " 727: 'test',\n",
       " 728: 'stop',\n",
       " 729: 'body',\n",
       " 730: 'deal',\n",
       " 731: 'dod',\n",
       " 732: 'comp',\n",
       " 733: 'killed',\n",
       " 734: 'brian',\n",
       " 735: 'head',\n",
       " 736: 'drivers',\n",
       " 737: '1992',\n",
       " 738: 'dead',\n",
       " 739: 'light',\n",
       " 740: 'needed',\n",
       " 741: 'clear',\n",
       " 742: 'third',\n",
       " 743: 'self',\n",
       " 744: '75u',\n",
       " 745: 'goes',\n",
       " 746: 'uses',\n",
       " 747: 'mode',\n",
       " 748: 'taken',\n",
       " 749: 'posted',\n",
       " 750: 'written',\n",
       " 751: 'bus',\n",
       " 752: 'business',\n",
       " 753: 'common',\n",
       " 754: 'except',\n",
       " 755: 'pittsburgh',\n",
       " 756: 'radio',\n",
       " 757: 'tin',\n",
       " 758: 'thus',\n",
       " 759: 'western',\n",
       " 760: 'fine',\n",
       " 761: '3t',\n",
       " 762: 'christ',\n",
       " 763: 'value',\n",
       " 764: 'turn',\n",
       " 765: 'short',\n",
       " 766: 'opinion',\n",
       " 767: 'week',\n",
       " 768: 'usenet',\n",
       " 769: 'communications',\n",
       " 770: 'advance',\n",
       " 771: 'return',\n",
       " 772: 'young',\n",
       " 773: 'myself',\n",
       " 774: 'exist',\n",
       " 775: 'air',\n",
       " 776: 'bob',\n",
       " 777: 'become',\n",
       " 778: 'force',\n",
       " 779: 'position',\n",
       " 780: 'mouse',\n",
       " 781: 'guy',\n",
       " 782: 'build',\n",
       " 783: 'se',\n",
       " 784: 'political',\n",
       " 785: 'among',\n",
       " 786: 'os',\n",
       " 787: 'happened',\n",
       " 788: '2di',\n",
       " 789: 'easy',\n",
       " 790: 'stanford',\n",
       " 791: 'laws',\n",
       " 792: 'form',\n",
       " 793: 'smith',\n",
       " 794: 'society',\n",
       " 795: 'mine',\n",
       " 796: 'particular',\n",
       " 797: 'runs',\n",
       " 798: 'anti',\n",
       " 799: 'discussion',\n",
       " 800: 'cleveland',\n",
       " 801: 'monitor',\n",
       " 802: 'sci',\n",
       " 803: 'motif',\n",
       " 804: 'date',\n",
       " 805: 'peter',\n",
       " 806: 'four',\n",
       " 807: 'ed',\n",
       " 808: 'whatever',\n",
       " 809: 'fire',\n",
       " 810: 'built',\n",
       " 811: 'corporation',\n",
       " 812: '04',\n",
       " 813: 'drives',\n",
       " 814: 'interesting',\n",
       " 815: 'hear',\n",
       " 816: 'rest',\n",
       " 817: 'therefore',\n",
       " 818: \"let's\",\n",
       " 819: 'needs',\n",
       " 820: 'military',\n",
       " 821: '41',\n",
       " 822: 'books',\n",
       " 823: 'hit',\n",
       " 824: 'league',\n",
       " 825: 'study',\n",
       " 826: 'night',\n",
       " 827: 'rate',\n",
       " 828: 'model',\n",
       " 829: 'gets',\n",
       " 830: 'lost',\n",
       " 831: 'face',\n",
       " 832: 'considered',\n",
       " 833: \"haven't\",\n",
       " 834: 'future',\n",
       " 835: 'request',\n",
       " 836: 'past',\n",
       " 837: '44',\n",
       " 838: 'itself',\n",
       " 839: 'east',\n",
       " 840: 'strong',\n",
       " 841: \"what's\",\n",
       " 842: 'situation',\n",
       " 843: 'early',\n",
       " 844: 'front',\n",
       " 845: 'cars',\n",
       " 846: 'bhj',\n",
       " 847: 'job',\n",
       " 848: 'statement',\n",
       " 849: 'special',\n",
       " 850: 'red',\n",
       " 851: 'texas',\n",
       " 852: 'anonymous',\n",
       " 853: 'longer',\n",
       " 854: 'ground',\n",
       " 855: 'la',\n",
       " 856: 'crime',\n",
       " 857: 'manager',\n",
       " 858: 'section',\n",
       " 859: 'women',\n",
       " 860: \"aren't\",\n",
       " 861: 'ones',\n",
       " 862: 'scott',\n",
       " 863: 'personal',\n",
       " 864: \"wasn't\",\n",
       " 865: 'thinking',\n",
       " 866: 'effect',\n",
       " 867: 'users',\n",
       " 868: 'kill',\n",
       " 869: '36',\n",
       " 870: 'acs',\n",
       " 871: 'weapons',\n",
       " 872: 'due',\n",
       " 873: 'add',\n",
       " 874: 'water',\n",
       " 875: 'sell',\n",
       " 876: 'plus',\n",
       " 877: 'upon',\n",
       " 878: 'act',\n",
       " 879: 'policy',\n",
       " 880: 'road',\n",
       " 881: 'looks',\n",
       " 882: 'cd',\n",
       " 883: 'chicago',\n",
       " 884: 'baseball',\n",
       " 885: 'ii',\n",
       " 886: 'format',\n",
       " 887: 'sometimes',\n",
       " 888: '60',\n",
       " 889: 'series',\n",
       " 890: 'caltech',\n",
       " 891: 'cso',\n",
       " 892: 'offer',\n",
       " 893: 'process',\n",
       " 894: 'defense',\n",
       " 895: 'error',\n",
       " 896: 'assume',\n",
       " 897: 'members',\n",
       " 898: 'specific',\n",
       " 899: 'disclaimer',\n",
       " 900: 'federal',\n",
       " 901: 'entry',\n",
       " 902: 'project',\n",
       " 903: 'coming',\n",
       " 904: 'tom',\n",
       " 905: 'legal',\n",
       " 906: 'recently',\n",
       " 907: 'sgi',\n",
       " 908: '38',\n",
       " 909: 'richard',\n",
       " 910: 'includes',\n",
       " 911: 'religious',\n",
       " 912: 'accept',\n",
       " 913: 'soon',\n",
       " 914: 'peace',\n",
       " 915: 'worth',\n",
       " 916: 'united',\n",
       " 917: 'media',\n",
       " 918: 'privacy',\n",
       " 919: 'ram',\n",
       " 920: 'voice',\n",
       " 921: 'behind',\n",
       " 922: 'rutgers',\n",
       " 923: 'nor',\n",
       " 924: 'food',\n",
       " 925: 'controller',\n",
       " 926: 'bitnet',\n",
       " 927: 'tv',\n",
       " 928: 'taking',\n",
       " 929: 'happen',\n",
       " 930: 'frank',\n",
       " 931: 'according',\n",
       " 932: 'near',\n",
       " 933: 'cases',\n",
       " 934: 'together',\n",
       " 935: 'along',\n",
       " 936: 'technical',\n",
       " 937: 'further',\n",
       " 938: 'moral',\n",
       " 939: 'giz',\n",
       " 940: 'comments',\n",
       " 941: 'international',\n",
       " 942: 'related',\n",
       " 943: 'purpose',\n",
       " 944: 'parts',\n",
       " 945: 'total',\n",
       " 946: 'move',\n",
       " 947: 'allow',\n",
       " 948: 'fbi',\n",
       " 949: 'cut',\n",
       " 950: 'corp',\n",
       " 951: 'fi',\n",
       " 952: 'expect',\n",
       " 953: 'explain',\n",
       " 954: 'land',\n",
       " 955: 'performance',\n",
       " 956: 'austin',\n",
       " 957: 'sources',\n",
       " 958: 'previous',\n",
       " 959: 'design',\n",
       " 960: 'knowledge',\n",
       " 961: 'months',\n",
       " 962: 'action',\n",
       " 963: 'mentioned',\n",
       " 964: 'dr',\n",
       " 965: 'result',\n",
       " 966: 'math',\n",
       " 967: 'ny',\n",
       " 968: 'uunet',\n",
       " 969: 'half',\n",
       " 970: 'friend',\n",
       " 971: 'medical',\n",
       " 972: '42',\n",
       " 973: 'various',\n",
       " 974: 'court',\n",
       " 975: 'utexas',\n",
       " 976: 'close',\n",
       " 977: 'thomas',\n",
       " 978: 'necessary',\n",
       " 979: 'building',\n",
       " 980: '02',\n",
       " 981: '75',\n",
       " 982: 'nhl',\n",
       " 983: 'reference',\n",
       " 984: 'involved',\n",
       " 985: 'sent',\n",
       " 986: 'images',\n",
       " 987: 'cover',\n",
       " 988: 'ide',\n",
       " 989: 'nl',\n",
       " 990: 'currently',\n",
       " 991: 'hold',\n",
       " 992: 'response',\n",
       " 993: 'himself',\n",
       " 994: 'million',\n",
       " 995: 'north',\n",
       " 996: 'chris',\n",
       " 997: 'machines',\n",
       " 998: 'rules',\n",
       " 999: 'univ',\n",
       " 1000: '1993apr15',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word #list words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134142 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  4 11 ... 16 18  4]\n"
     ]
    }
   ],
   "source": [
    "#categorical data -> target column\n",
    "t = target\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(t)\n",
    "y_train = encoder.transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(target[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Keras Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               10240512  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 10,513,428\n",
      "Trainable params: 10,513,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 25s 2ms/step - loss: 0.7901 - acc: 0.7966 - val_loss: 0.3582 - val_acc: 0.9125\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 37s 4ms/step - loss: 0.0750 - acc: 0.9868 - val_loss: 0.4363 - val_acc: 0.8966\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 32s 3ms/step - loss: 0.0383 - acc: 0.9951 - val_loss: 0.3745 - val_acc: 0.9108\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 31s 3ms/step - loss: 0.0211 - acc: 0.9979 - val_loss: 0.3734 - val_acc: 0.9152\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 32s 3ms/step - loss: 0.0412 - acc: 0.9946 - val_loss: 0.4566 - val_acc: 0.9055\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 37s 4ms/step - loss: 0.0317 - acc: 0.9967 - val_loss: 0.4689 - val_acc: 0.8958\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0287 - acc: 0.9971 - val_loss: 0.4940 - val_acc: 0.8966\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 31s 3ms/step - loss: 0.0246 - acc: 0.9973 - val_loss: 0.4209 - val_acc: 0.9170\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 36s 4ms/step - loss: 0.0191 - acc: 0.9980 - val_loss: 0.4056 - val_acc: 0.9178\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 32s 3ms/step - loss: 0.0185 - acc: 0.9982 - val_loss: 0.4131 - val_acc: 0.9178\n"
     ]
    }
   ],
   "source": [
    "#First Trial\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512, activation='relu', input_shape=(vocab_size,)))\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dense(20, activation='softmax'))\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting model\n",
    "history1 = model1.fit(x_trains, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               10240512  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 10,513,428\n",
      "Trainable params: 10,513,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Second Trial\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(vocab_size,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 25s 2ms/step - loss: 1.0484 - acc: 0.7260 - val_loss: 0.3701 - val_acc: 0.9108\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 29s 3ms/step - loss: 0.1071 - acc: 0.9771 - val_loss: 0.3890 - val_acc: 0.9170\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 29s 3ms/step - loss: 0.0584 - acc: 0.9917 - val_loss: 0.4128 - val_acc: 0.9143\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 29s 3ms/step - loss: 0.0471 - acc: 0.9948 - val_loss: 0.4198 - val_acc: 0.9143\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0398 - acc: 0.9954 - val_loss: 0.5029 - val_acc: 0.9072\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 28s 3ms/step - loss: 0.0441 - acc: 0.9946 - val_loss: 0.4466 - val_acc: 0.9090\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0321 - acc: 0.9966 - val_loss: 0.4250 - val_acc: 0.9196\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 36s 4ms/step - loss: 0.0228 - acc: 0.9975 - val_loss: 0.4387 - val_acc: 0.9152\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 31s 3ms/step - loss: 0.0237 - acc: 0.9978 - val_loss: 0.4347 - val_acc: 0.9134\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 32s 3ms/step - loss: 0.0287 - acc: 0.9967 - val_loss: 0.4454 - val_acc: 0.9125\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "history = model.fit(x_trains, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data preprocessing\n",
    "twenty_test = load_files('/home/hadoop/scikit_learn_data/20news_home/20news-bydate-test',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test = twenty_test.data # Extract text and target\n",
    "target_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_test = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer_test.fit_on_texts(texts_test)\n",
    "\n",
    "x_test = tokenizer.texts_to_matrix(texts_test, mode='tfidf')\n",
    "y_test = encoder.transform(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532/7532 [==============================] - 2s 204us/step\n",
      "Test accuracy: 0.8325809879753733\n"
     ]
    }
   ],
   "source": [
    "#Do testing\n",
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Overfitting is happening = model acc: 0.9967, test acc: 0.832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predict the news class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check result\n",
    "import numpy as np\n",
    "predicted_label = np.argmax(prediction[0])\n",
    "actual_label = target_test[0]\n",
    "predicted_label_name = labels[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text content: From: stimpy@dev-null.phys.psu.edu (Gregory Nagy)\n",
      "Subject: Re: ESPN UP YOURS .........\n",
      "Organization: Penn State Laboratory for Elementary Steam Physics\n",
      "Lines: 52\n",
      "NNTP-Posting-Host: dev-null.phys.psu.edu\n",
      "\n",
      "In article <C5u542.3CD@news.udel.edu> tmavor@earthview.cms.udel.edu writes:\n",
      ">>\n",
      ">>[Various justifiable rantings on ESPN coverage by several deleted]\n",
      ">>\n",
      ">\n",
      ">The only way to change ESPN's thinking, if it is even possible, is to complain\n",
      ">to them directly.  Anyone know there telephone # in Bristol, Ct?  \n",
      "\n",
      "Heh... Try the rec.autos.sport FAQ. They are always calling ESPN to complain.\n",
      "I'm sure you could find the number for ABC there too, as many west-coast \n",
      "viewers were compaining about how something as boring as hockey cut into\n",
      "the Long Beach GP. =)\n",
      "\n",
      ">\n",
      ">I do find it hard to believe that ESPN doesn't think viewers will simply\n",
      ">change the channel from a boring game....I know I did.  And then, when\n",
      ">they didn't show the NYI-Wash overtime(s), I was livid!  If I wanted\n",
      ">to watch baseball, I could have turned on the Phillies-Padres extra\n",
      ">inning game....instead, I went to bed angry......I boycotted ESPN's\n",
      ">morning Sportscenter today, I was still so incensed.\n",
      "\n",
      "Were you (and several of the other people here it seems) asleep the day\n",
      "\"contracts\" were explained? ASPN has a piece of paper saying it MUST\n",
      "show that baseball game if it happens. Many businesses payedd money to\n",
      "have their commercials run during a baseball game. This is a business,\n",
      "not your own personal video servant.\n",
      "\n",
      ">\n",
      ">My wife says I shouldn't go to bed angry, but last nite.........GRRRRRRR!\n",
      ">\n",
      "\n",
      "Maybe you should put that anger into something positive. For example, I saw\n",
      "ads for the new Dodge both on the ESPN and KBL broadcasts. Why not write to\n",
      "Dodge saying that \"thanks to the ads run during the STANLEY CUP PLAYOFFS, \n",
      "you will now concider their products in the future. They love to hear stuff\n",
      "like that and in the future will be more willing to buy commercial time\n",
      "for hockey games, giving ESPN (and other networks) more incentive to carry\n",
      "games (just one example)\n",
      "\n",
      "Come on people, as great as we think it is, Hockey does not leapfrog the\n",
      "\"big three\" overight.\n",
      "\n",
      "> \n",
      ">---------------------------------------------------------------------\n",
      ">Tim Mavor\t\t   |  \"I am known by many names.......\n",
      ">College of Marine Studies  |   some call me.........Tim.\"\n",
      ">Univ. of Delaware\t   |    \n",
      ">Newark, DE 19716\t   |  \"You know much that is hidden, O' Tim!\"\n",
      ">tmavor@pandora.cms.udel.edu|  \tMonty Python and the Holy Grail\t---------------------------------------------------------------------\n",
      ">\n",
      "\n",
      "\n",
      "\n",
      "predicted_label: 10\n",
      "actual_label: 10\n",
      "label_name: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print(\"text content:\", texts_test[0])\n",
    "print(\"predicted_label:\", predicted_label)\n",
    "print(\"actual_label:\", actual_label)\n",
    "print(\"label_name:\", predicted_label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prediction Metric</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "predict_test = []\n",
    "for i in range(len(prediction)):\n",
    "    predict_test.append(np.argmax(prediction[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257   1   0   1   0   1   0   0   1   0   0   1   0   4   4  19   0   0\n",
      "    2  28]\n",
      " [  0 302   9  16   7  19   5   1   0   2   0   4  10   0   4   4   1   1\n",
      "    0   4]\n",
      " [  0  24 276  39  11  13   6   0   0   1   1   2   1   1   5   2   0   2\n",
      "    7   3]\n",
      " [  0  12  19 301  22   2  12   2   0   0   0   1  15   0   2   1   1   1\n",
      "    1   0]\n",
      " [  1   4   5  20 316   3  16   1   0   0   0   0   7   4   2   1   3   0\n",
      "    1   1]\n",
      " [  1  34  17   2   2 329   3   0   1   0   0   0   0   1   3   0   0   1\n",
      "    1   0]\n",
      " [  0   1   1  11   8   1 348   5   1   0   1   0   9   2   0   0   1   0\n",
      "    0   1]\n",
      " [  0   2   1   5   3   0   9 348  12   1   2   0   3   3   2   0   0   0\n",
      "    3   2]\n",
      " [  0   1   0   0   1   2   4   4 381   0   0   0   0   2   0   0   0   0\n",
      "    2   1]\n",
      " [  0   0   1   0   0   0   0   1   1 369  18   0   0   0   0   1   0   2\n",
      "    4   0]\n",
      " [  0   1   0   0   1   1   0   0   0   8 384   0   0   0   0   0   1   2\n",
      "    1   0]\n",
      " [  1   3   1   3   2   1   4   1   0   1   0 369   2   1   0   0   2   1\n",
      "    4   0]\n",
      " [  6  13   9  34   9   2   9   5   2   0   0  18 256   9   8   1   1   1\n",
      "    7   3]\n",
      " [  3   7   1   5   1   2   8   2   2   3   0   0   4 340   0   5   1   5\n",
      "    5   2]\n",
      " [  1  15   0   2   0   2   2   1   0   0   0   2   2   8 350   3   1   0\n",
      "    5   0]\n",
      " [  5   2   1   1   0   0   0   1   0   1   0   0   1   2   3 364   0   2\n",
      "    0  15]\n",
      " [  2   1   0   0   1   1   4   2   4   3   0   5   0   1   7   1 285   3\n",
      "   30  14]\n",
      " [  6   2   1   0   0   2   0   0   1   3   1   0   0   1   1  11   0 330\n",
      "   16   1]\n",
      " [  4   2   0   0   2   0   4   1   1   0   1   3   0   8   5   4  51   7\n",
      "  199  18]\n",
      " [ 28   1   3   1   0   0   1   1   0   0   0   0   0   4   2  26   6   5\n",
      "    6 167]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so far so goooood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
